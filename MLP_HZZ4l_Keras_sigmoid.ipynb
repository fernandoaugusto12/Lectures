{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MLP_HZZ4l_Keras_sigmoid.ipynb","provenance":[{"file_id":"https://github.com/thongonary/machine_learning_vbscan/blob/master/2-dense.ipynb","timestamp":1551261043668}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"H6Ol7ljj2srK"},"source":["# MLP with sigmoid output for HZZ4l analysis using  Keras\n","Authors: Javier Duarte, Thong Nguyen     \n","Modified: Andre Sznajder"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k45E5Zujeafn"},"source":[""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UHbrwZrsYSN5"},"source":["## **Mount Google Drive**"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1569350980455,"user_tz":180,"elapsed":26503,"user":{"displayName":"Andre Sznajder","photoUrl":"https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg","userId":"12562331206892861623"}},"id":"LZuut1Rp2_rw","outputId":"5335e735-e469-467d-ad44-b0fc15686686","colab":{"base_uri":"https://localhost:8080/","height":153}},"source":["!fusermount -u drive\n","!pip install uproot\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","root_dir = '/content/gdrive/My Drive/'\n","data_dir = '/content/gdrive/My Drive/Colab Notebooks/Data/'\n","!ls 'Colab Notebooks/Data'"],"execution_count":17,"outputs":[{"output_type":"stream","text":["fusermount: failed to unmount /content/drive: No such file or directory\n","Requirement already satisfied: uproot in /usr/local/lib/python3.6/dist-packages (3.10.1)\n","Requirement already satisfied: cachetools in /usr/local/lib/python3.6/dist-packages (from uproot) (3.1.1)\n","Requirement already satisfied: awkward<1.0,>=0.12.0 in /usr/local/lib/python3.6/dist-packages (from uproot) (0.12.10)\n","Requirement already satisfied: uproot-methods>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from uproot) (0.7.1)\n","Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from uproot) (1.16.5)\n","Mounted at /content/gdrive\n","ls: cannot access 'Colab Notebooks/Data': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"qoRG_Nqp2srN"},"source":["#Load data using PANDAS data frames#\n","Now we load two different `NumPy` arrays. One corresponding to the VBF H->ZZ->4l signal and the other one corresponds to the QCD ZZ->4l background ."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1569350980643,"user_tz":180,"elapsed":26683,"user":{"displayName":"Andre Sznajder","photoUrl":"https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg","userId":"12562331206892861623"}},"id":"2QpkEpeC2srP","outputId":"ffc13b1f-e0ca-4729-a924-540862d5c918","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import uproot\n","import numpy as np\n","import pandas as pd\n","import h5py\n","\n","# fix random seed for reproducibility\n","seed = 7\n","np.random.seed(seed)\n","\n","filename = {}\n","upfile = {}\n","data = {}\n","df = {}\n","\n","#data_dir = '/Users/sznajder/cernbox/Data/2017ReducedTrees/histos2e2mu_25ns/'\n","#filename['sig'] = data_dir+'output_VBF_HToZZTo4L_M125_13TeV_powheg2_JHUGenV7011_pythia8_ext1.root'\n","#filename['bkg'] = data_dir+'bkg_merged_trees.root'\n","\n","#data_dir = '/Users/sznajder/Google Drive/Colab Notebooks/Data/'\n","filename['sig'] = data_dir+'ntuple_qqH_ZZ4mu.root'\n","filename['bkg'] = data_dir+'ntuple_bkg_ZZ4mu.root'\n","\n","treename = 'HZZ4LeptonsAnalysisReduced'\n","\n","upfile['sig'] = uproot.open(filename['sig'])\n","upfile['bkg'] = uproot.open(filename['bkg'])\n","\n","#upfile['bkg'] = uproot.iterate(,\"*.root\", treename)\n","\n","# Variables from Roottree to be copyed to PANDA dataframe\n","VARS = ['f_lept1_pt','f_lept1_eta','f_lept1_phi', \\\n","        'f_lept2_pt','f_lept2_eta','f_lept2_phi', \\\n","        'f_lept3_pt','f_lept3_eta','f_lept3_phi', \\\n","        'f_lept4_pt','f_lept4_eta','f_lept4_phi', \\\n","        'f_jet1_pt','f_jet1_eta','f_jet1_phi', \\\n","        'f_jet2_pt','f_jet2_eta','f_jet2_phi',\\\n","        'f_weight','f_mass4l','f_massjj','f_deltajj']\n","        \n","# Convert the Root branches/leaves to dictionaries of Numpy arrays \n","#data['sig'] = upfile['sig'][treename].arrays(VARS,namedecode=\"ascii\")\n","#data['bkg'] = upfile['bkg'][treename].arrays(VARS,namedecode=\"ascii\")\n","# Convert disctionary of arrays into Pandas dataframe\n","#df['sig'] = pd.DataFrame(data=data['sig'],columns=VARS)\n","#df['bkg'] = pd.DataFrame(data=data['bkg'],columns=VARS)\n","\n","# Convert the Root branches/leaves directly to Pandas dataframe \n","df['sig'] = upfile['sig'][treename].pandas.df(VARS)\n","df['bkg'] = upfile['bkg'][treename].pandas.df(VARS)\n","\n","# Crop the background sample to have the same number of events as the signal sample\n","#df['bkg']=df['bkg'][:len(df['sig'])]\n","\n","print(len(df['sig']))\n","print(len(df['bkg']))\n","\n","# Remove undefined variable entries VARS[i] <= -999\n","#for i in range(len(VARS)): \n","#  df['sig'] = df['sig'][(df['sig'][VARS[i]] > -999)]\n","#  df['bkg']= df['bkg'][(df['bkg'][VARS[i]] > -999)]\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["25817\n","58107\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Hz1t3gbySwZo"},"source":["## **Specify the number of NN input variables **"]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"ok","timestamp":1569350980644,"user_tz":180,"elapsed":26677,"user":{"displayName":"Andre Sznajder","photoUrl":"https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg","userId":"12562331206892861623"}},"id":"kVEM_lk0S6rx","outputId":"32fb0201-9b03-4a1b-9e59-a15a0c0e7ee6","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\n","# Define the NN input variables ( don't use  'f_weight', 'f_mass4l','f_massjj','f_deltajj' as inputs ! )\n","#NINPUT = len(VARS)-1\n","NINPUT = len(VARS)-4\n","print(\"NINPUT=\",NINPUT)\n","\n"],"execution_count":19,"outputs":[{"output_type":"stream","text":["NINPUT= 18\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4CfVc04A2srW"},"source":["## Define the model using KERAS functional API \n","We'll start with a dense (fully-connected) NN layer.\n","Our model will have a single fully-connected hidden layer with the same number of neurons as input variables. \n","The output layer contains a single neuron using a sigmoid activation in order to a number between 0 and 1 to make binary classification\n","\n","We are using the `binary_crossentropy` loss function during training, a standard loss function for binary classification problems. \n","We will optimize the model with the Adam algorithm for stochastic gradient descent and we will collect accuracy metrics while the model is trained."]},{"cell_type":"code","metadata":{"colab_type":"code","executionInfo":{"status":"error","timestamp":1569350981103,"user_tz":180,"elapsed":27129,"user":{"displayName":"Andre Sznajder","photoUrl":"https://lh3.googleusercontent.com/-Bujzmul3q4w/AAAAAAAAAAI/AAAAAAAAA30/Zzdg4zcPB-8/s64/photo.jpg","userId":"12562331206892861623"}},"id":"EwjJCwoy2srX","outputId":"1ffcadb8-60d4-4e01-f430-b1f6abba1c42","scrolled":false,"colab":{"base_uri":"https://localhost:8080/","height":395}},"source":["# baseline keras model\n","from keras.models import Sequential, Model\n","from keras.optimizers import SGD, Adam\n","from keras.layers import Input, Activation, Dense, Dropout\n","from keras.utils import np_utils\n","\n","# Select the NN input variables. Using just lepton and and jets 4-mom ( low level veriables) ! \n","input  = Input(shape=(NINPUT,), name = 'input') \n","hidden = Dense(NINPUT*3 , name = 'hidden', kernel_initializer='normal', activation='relu')(input)\n","hidden = Dropout(rate=0.2)(hidden)\n","output  = Dense(1      , name = 'output', kernel_initializer='normal', activation='sigmoid')(hidden)\n","#hidden1 = Dense(NINPUT  , name = 'hidden1', kernel_initializer='normal', activation='relu')(input)\n","#hidden1 = Dropout(rate=0.2)(hidden1)\n","#hidden2 = Dense(int(NINPUT/2)  , name = 'hidden2', kernel_initializer='normal', activation='relu')(hidden1)\n","#hidden2 = Dropout(rate=0.2)(hidden2)\n","#hidden3 = Dense(int(NINPUT/4)  , name = 'hidden3', kernel_initializer='normal', activation='relu')(hidden2)\n","#hidden3 = Dropout(rate=0.2)(hidden3)\n","#output  = Dense(1      , name = 'output', kernel_initializer='normal', activation='sigmoid')(hidden3)\n","\n","# create the model\n","model = Model(inputs=input, outputs=output)\n","# Define the optimizer ( minimization algorithm )\n","#optim = SGD(lr=0.01,decay=1e-6)\n","optim = Adam()\n","# compile the model\n","#model.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'])\n","model.compile(optimizer=optim, loss='cross_entropy', metrics=['accuracy'])\n","# print the model summary\n","model.summary()"],"execution_count":20,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-6ed91f861bff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# compile the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#model.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cross_entropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;31m# print the model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0mloss_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m             \u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mloss_function\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(identifier)\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0midentifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdeserialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midentifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/losses.py\u001b[0m in \u001b[0;36mdeserialize\u001b[0;34m(name, custom_objects)\u001b[0m\n\u001b[1;32m    112\u001b[0m                                     \u001b[0mmodule_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                                     \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                                     printable_module_name='loss function')\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mdeserialize_keras_object\u001b[0;34m(identifier, module_objects, custom_objects, printable_module_name)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 raise ValueError('Unknown ' + printable_module_name +\n\u001b[0;32m--> 167\u001b[0;31m                                  ':' + function_name)\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unknown loss function:cross_entropy"]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Uhb0N4yX2sre"},"source":["## Dividing the data into testing and training dataset and renormalizing events weights\n","\n","We will split the data into two parts (one for training+validation and one for testing). \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"2Sa9Kxml2srf","colab":{}},"source":["# Add the variable isSignal to the dataframe containing the signal and background events\n","df['sig']['isSignal'] = np.ones(len(df['sig'])) \n","df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) \n","\n","\n","# Renormalizes the events weights to give unit sum in the signal and background dataframes\n","# This is necessary for the NN to learn signal and background in the same proportion , \n","#   independently of number of events and absolute weights of events in each sample !\n","df['sig']['f_weight']=df['sig']['f_weight']/df['sig']['f_weight'].sum(axis=0)\n","df['bkg']['f_weight']=df['bkg']['f_weight']/df['bkg']['f_weight'].sum(axis=0)\n","\n","\n","# Concatanate the signal and background in a single  data frames \n","df_all = pd.concat([df['sig'],df['bkg']])\n","\n","# Transform the data frame into a 2-dim numpy array and get the target output ( last entry )\n","NVARS=len(VARS)\n","print(\"NVARS=\",NVARS)\n","dataset = df_all.values\n","\n","# Random shuffles the dataset to mix signal and background events before splitting between train and test samples\n","np.random.shuffle(dataset)\n","\n","X = dataset[:,0:NVARS]\n","Y = dataset[:,-1]\n","\n","print(\"X=\",X)\n","print(\"Y=\",Y)\n","\n","# Split the sample into train and test  \n","from sklearn.model_selection import train_test_split\n","X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True)\n","\n","# Get events weights\n","weights=X_train_val[:,18]\n","\n","print(\"Signal Max Weight=\",df['sig']['f_weight'].max(axis=0))\n","print(\"Signal Min Weight=\",df['sig']['f_weight'].min(axis=0))\n","print(\"Background Max Weight=\",df['bkg']['f_weight'].max(axis=0))\n","print(\"Background Min Weight=\",df['bkg']['f_weight'].min(axis=0))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MGYtcQjF2srj"},"source":["## Run NN Training \n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"JFDgmRSn2srk","scrolled":false,"colab":{}},"source":["# Number of training epochs\n","nepochs=100 \n","# Batch size\n","batch=100\n","# Train classifier\n","history = model.fit(X_train_val[:,0:NINPUT], \n","                    Y_train_val,\n","                    epochs=nepochs, \n","                    sample_weight=weights,\n","                    batch_size=batch, \n","                    verbose=1, # switch to 1 for more verbosity \n","                    validation_split=0.3)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"h9fB0MiA2sro"},"source":["## Plot performance\n","Here, we plot the history of the training and the performance in a ROC curve"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ubEbdlSx2srp","colab":{}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')\n","plt.figure(figsize=(15,10))\n","\n","# plot loss vs epoch\n","ax = plt.subplot(2, 2, 1)\n","ax.plot(history.history['loss'], label='loss')\n","ax.plot(history.history['val_loss'], label='val_loss')\n","#ax.set_ylim([0, 1])\n","ax.legend(loc=\"upper right\")\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Loss')\n","\n","# plot accuracy vs epoch\n","ax = plt.subplot(2, 2, 2)\n","ax.plot(history.history['acc'], label='acc')\n","ax.plot(history.history['val_acc'], label='val_acc')\n","ax.set_ylim([0, 1.0])\n","ax.legend(loc=\"upper left\")\n","ax.set_xlabel('Epoch')\n","ax.set_ylabel('Accuracy')\n","\n","# Plot ROC\n","Y_predict = model.predict(X_test[:,0:NINPUT])\n","from sklearn.metrics import roc_curve, auc\n","fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n","roc_auc = auc(fpr, tpr)\n","ax = plt.subplot(2, 2, 3)\n","ax.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n","ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n","ax.set_xlim([0, 1.0])\n","ax.set_ylim([0, 1.0])\n","ax.set_xlabel('False Positive Rate(FPR)')\n","ax.set_ylabel('True Positive Rate(TPR)')\n","ax.set_title('Receiver Operating Curve(ROC)')\n","ax.legend(loc=\"lower right\")\n","\n","# Plot DNN output \n","ax = plt.subplot(2, 2, 4)\n","X = np.linspace(0.0, 1.0, 100)\n","ax.hist(Y_predict, bins=X, label='bkg',histtype='step')\n","#ax.hist(Y_train_val, bins=X, label='bkg',histtype='step')\n","\n","ax.set_xlabel('DNN Output')\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"cUyd9EXL2sry"},"source":["# Plot  $m_{jj}$, $\\eta_{j}$ and $m_{4l}$  for NN output > cut to show that the NN did learned the physics !\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"B6HCm7PL2srz","colab":{}},"source":["\n","import matplotlib.pyplot as plt\n","\n","# Define the data frame for NN predictions and high level variables\n","data = pd.DataFrame({'f_mass4l':X_test[:,19],'f_massjj':X_test[:,20],'f_deltajj':X_test[:,21],'isSignal':Y_test[:],'NNoutput':Y_predict[:,0]})\n","\n","\n","# Selects events with NNoutput > cut\n","cut = 0.5 \n","data_sel   = data[(data['NNoutput'] >= cut)]\n","data_TP    = data[(data['NNoutput'] >= cut) & (data['isSignal'] == 1)]\n","data_unsel = data[(data['NNoutput'] < cut)]\n","data_TN    = data[(data['NNoutput'] < cut) & (data_unsel['isSignal'] == 0)]\n","\n","TP = len(data_TP)\n","FP = len(data_sel) - TP\n","TN = len(data_TN)\n","FN = len(data_unsel) - TN\n","\n","truepositiverate = float(TP)/(TP+FN)\n","fakepositiverate = float(FP)/(FP+FN)\n","print(truepositiverate)\n","print(fakepositiverate)\n","    \n","# Plot  delta_eta between jets for signal, background and NN selected events \n","plt.style.use('default') # It's ugly otherwise\n","plt.figure(figsize=(16, 3),dpi=100)\n","\n","plt.subplot(1, 3, 1)\n","plt.xlabel('deltajj')\n","X = np.linspace(0.0,10.,100)\n","df['bkg']['f_deltajj'].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n","df['sig']['f_deltajj'].plot.hist(bins=X, label='signal',histtype='step', density=1)\n","data_sel['f_deltajj'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n","plt.legend(loc='upper right')\n","plt.xlim(0,10)\n","\n","# Plot  dijets mass for signal, background and NN selected events \n","plt.subplot(1, 3, 2)\n","plt.xlabel('massjj')\n","X = np.linspace(0.0,1000.,100)\n","df['bkg']['f_massjj'].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n","df['sig']['f_massjj'].plot.hist(bins=X, label='signal',histtype='step', density=1)\n","data_sel['f_massjj'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n","plt.legend(loc='upper right')\n","plt.xlim(0,1000)\n","\n","# Plot 4l invariant mass for signal, background and NN selected events \n","plt.subplot(1, 3, 3)\n","plt.xlabel('mass4l')\n","X = np.linspace(50.,400.,100)\n","df['bkg']['f_mass4l'].plot.hist(bins=X, label='bkg',histtype='step', density=1)\n","df['sig']['f_mass4l'].plot.hist(bins=X, label='signal',histtype='step', density=1)\n","data_sel['f_mass4l'].plot.hist(bins=X, label='NN',histtype='step', density=1)\n","plt.legend(loc='upper right')\n","plt.xlim(50,400)\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rAahonCp2sr3"},"source":["\n","**Question 1:** What happens if you increase/decrease the number of hidden layers ?\n","\n","\n","\n","**Question 2:** What happens if you increase/decrease the number of nodes per hidden layer ?\n","\n","\n","\n","**Question 3:** What happens if you remove dropout ?\n","\n","\n","\n","**Question 4:** What happens if you change the batch size  ?\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fTsxswxe2sr9"},"source":[""]}]}