{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolution_padding_stride.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Lectures/blob/master/Convolution_padding_stride.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCfg4r92VK_f",
        "colab_type": "text"
      },
      "source": [
        "**Convolutional Layer**\n",
        "\n",
        "https://machinelearningmastery.com/padding-and-stride-for-convolutional-neural-networks/\n",
        "\n",
        "\n",
        "In a convolutional neural network, a convolutional layer is responsible for the systematic application of one or more filters to an input.\n",
        "\n",
        "The multiplication of a filter to the input image results in a single output. The input is typically three-dimensional images (e.g. rows, columns and channels), and in turn, the filters are also three-dimensional with the same number of channels and fewer rows and columns than the input image. As such, the filter is repeatedly applied to each part of the input image, resulting in a two-dimensional output map of activations, called a feature map.\n",
        "\n",
        "The filter contains the weights that must be learned during the training of the layer. The filter weights represent the structure or feature that the filter will detect and the strength of the activation indicates the degree to which the feature was detected.\n",
        "\n",
        "We can demonstrate this with a small example. In this example, we define a single input image or sample that has one channel and is an eight pixel by eight pixel square with all 0 values and a two-pixel wide vertical line in the center.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWVMj1BbU1NH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "100a6eec-3213-4e0e-9327-eeb367128cf6"
      },
      "source": [
        "# example of using a single Keras convolutional layer\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "\n",
        "# define input data\n",
        "data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n",
        "data = asarray(data)\n",
        "data = data.reshape(1, 8, 8, 1)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLRhvDb_V1Ub",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next, we define a model that expects input samples to have the shape (8,8,1) and has a single hidden convolutional layer with a single filter with the shape of three pixels by three pixels.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXsvNcBPV9VN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "a91a43b1-65e6-467a-fc29-d062e99c2631"
      },
      "source": [
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1112 21:28:06.884246 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W1112 21:28:06.902502 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W1112 21:28:06.910237 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 6, 6, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQsyF-gSWIQ7",
        "colab_type": "text"
      },
      "source": [
        "The filter is initialized with random weights as part of the initialization of the model. We will overwrite the random weights and hard code our own 3×3 filter that will detect vertical lines.\n",
        "\n",
        "That is the filter will strongly activate when it detects a vertical line and weakly activate when it does not. We expect that by applying this filter across the input image, the output feature map will show that the vertical line was detected.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BnwwywoWNpm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "b0d4e09a-275d-44ef-8887-e391920d2ba4"
      },
      "source": [
        "# define a filter to detect a vertical line\n",
        "filter   = [[[[0]],[[1]],[[0]]],\n",
        "            [[[0]],[[1]],[[0]]],\n",
        "            [[[0]],[[1]],[[0]]]]\n",
        "weights = [asarray(filter), asarray([0.0])]\n",
        "# store the weights in the model\n",
        "model.set_weights(weights)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1112 21:28:06.977031 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W1112 21:28:06.978396 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W1112 21:28:06.979583 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W1112 21:28:07.093233 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "W1112 21:28:07.094469 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "W1112 21:28:07.284297 140392327108480 module_wrapper.py:139] From /usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT_u3l6PW16L",
        "colab_type": "text"
      },
      "source": [
        "Apply the filter to our input image by calling the predict() function on the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV5qOsuuW-vZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# apply filter to input data\n",
        "yhat = model.predict(data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml7N8GHpXC91",
        "colab_type": "text"
      },
      "source": [
        "The result is a four-dimensional output with one batch, a given number of rows and columns, and one filter, or [batch, rows, columns, filters].\n",
        "\n",
        "We can print the activations in the single feature map to confirm that the line was detected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quO1_zrNXHQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "88c70aa0-afbb-44dc-b9d5-e3bcb31c3d41"
      },
      "source": [
        "# enumerate rows\n",
        "for r in range(yhat.shape[1]):\n",
        "\t# print each column in the row\n",
        "\tprint([yhat[0,r,c,0] for c in range(yhat.shape[2])])\n",
        " "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n",
            "[0.0, 0.0, 3.0, 3.0, 0.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uzbWttkXiIA",
        "colab_type": "text"
      },
      "source": [
        "Of note is that the single hidden convolutional layer will take the 8×8 pixel input image and will produce a feature map with the dimensions of 6×6. We will go into why this is the case in the next section.\n",
        "\n",
        "We can also see that the layer has 10 parameters, that is nine weights for the filter (3×3) and one weight for the bias.\n",
        "\n",
        "Finally, the feature map is printed. We can see from reviewing the numbers in the 6×6 matrix that indeed the manually specified filter detected the vertical line in the middle of our input image.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZpZUrBQXvRg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Problem of Border Effects**\n",
        "\n",
        "We defined a single filter with the size of three pixels high and three pixels wide (rows, columns).\n",
        "\n",
        "We saw that the application of the 3×3 filter, referred to as the kernel size in Keras, to the 8×8 input image resulted in a feature map with the size of 6×6.\n",
        "\n",
        "That is, the input image with 64 pixels was reduced to a feature map with 36 pixels. Where did the other 28 pixels go?\n",
        "\n",
        "The filter is applied like a sliding window over the input image. It starts at the top left corner of the image and is moved from left to right one pixel column at a time until the edge of the filter reaches the edge of the image.\n",
        "\n",
        "For a 3×3 pixel filter applied to a 8×8 input image, we can see that it can only be applied six times, resulting in the width of six in the output feature map.\n",
        "\n",
        "For example, let’s work through each of the six patches of the input image (left) dot product (“.” operator) the filter (right):\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYPy0UJSYccj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "742ababb-58a6-41d2-982e-a671ba664c18"
      },
      "source": [
        "0, 0, 0   0, 1, 0\n",
        "0, 0, 0 * 0, 1, 0 = 0\n",
        "0, 0, 0   0, 1, 0\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-45e1e8a82dac>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    0, 0, 0   0, 1, 0\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIGQ6HRHYvpE",
        "colab_type": "text"
      },
      "source": [
        "Moved right one pixel:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN0T9HQmYzO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0, 0, 1   0, 1, 0\n",
        "0, 0, 1 * 0, 1, 0 = 0\n",
        "0, 0, 1   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ_B6SUCdG4P",
        "colab_type": "text"
      },
      "source": [
        "Moved right one pixel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3liG3OodM8Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0, 1, 1   0, 1, 0\n",
        "0, 1, 1 * 0, 1, 0 = 3\n",
        "0, 1, 1   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ifm9vJ7JdYy5",
        "colab_type": "text"
      },
      "source": [
        "Moved right one pixel:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zz-M_mRldjI2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1, 1, 0   0, 1, 0\n",
        "1, 1, 0 * 0, 1, 0 = 3\n",
        "1, 1, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diMoKPOMdf-c",
        "colab_type": "text"
      },
      "source": [
        "Moved right one pixel:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITHi5eL5dkIH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1, 0, 0   0, 1, 0\n",
        "1, 0, 0 * 0, 1, 0 = 0\n",
        "1, 0, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ob5lqqKdhmD",
        "colab_type": "text"
      },
      "source": [
        "Moved right one pixel:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tigoOA7tdk76",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0, 0, 0   0, 1, 0\n",
        "0, 0, 0 * 0, 1, 0 = 0\n",
        "0, 0, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCVa34RBd6PD",
        "colab_type": "text"
      },
      "source": [
        "That gives us the first row and each column of the output feature map:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANyhAsABd7V9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0.0, 0.0, 3.0, 3.0, 0.0, 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlV9CstYd-Ud",
        "colab_type": "text"
      },
      "source": [
        "The reduction in the size of the input to the feature map is referred to as border effects. It is caused by the interaction of the filter with the border of the image.\n",
        "\n",
        "This is often not a problem for large images and small filters but can be a problem with small images. It can also become a problem once a number of convolutional layers are stacked.\n",
        "\n",
        "For example, below is the same model updated to have two stacked convolutional layers.\n",
        "\n",
        "This means that a 3×3 filter is applied to the 8×8 input image to result in a 6×6 feature map as in the previous section. A 3×3 filter is then applied to the 6×6 feature map."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndrQTbRueM3J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "bd1b5520-8650-4dc9-80d9-4773f034e84a"
      },
      "source": [
        "# example of stacked convolutional layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))\n",
        "model.add(Conv2D(1, (3,3)))\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 6, 6, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 4, 4, 1)           10        \n",
            "=================================================================\n",
            "Total params: 20\n",
            "Trainable params: 20\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cik9GCgYeQr5",
        "colab_type": "text"
      },
      "source": [
        "Running the example summarizes the shape of the output from each layer.\n",
        "\n",
        "We can see that the application of filters to the feature map output of the first layer, in turn, results in a smaller 4×4 feature map.\n",
        "\n",
        "This can become a problem as we develop very deep convolutional neural network models with tens or hundreds of layers. We will simply run out of data in our feature maps upon which to operate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKzw9Q8vesN8",
        "colab_type": "text"
      },
      "source": [
        "**Effect of the Filter Size**\n",
        "\n",
        "Different sized filters will detect different sized features in the input image and, in turn, will result in differently sized feature maps.\n",
        "\n",
        "It is common to use 3×3 sized filters, and perhaps 5×5 or even 7×7 sized filters, for larger input images.\n",
        "\n",
        "For example, below is an example of the model with a single filter updated to use a filter size of 5×5 pixels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZF9rALJe3Ou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "35657af0-1660-4c88-b422-19d2124e2dfd"
      },
      "source": [
        "# example of a convolutional layer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (5,5), input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_4 (Conv2D)            (None, 4, 4, 1)           26        \n",
            "=================================================================\n",
            "Total params: 26\n",
            "Trainable params: 26\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mxQ61BofCqD",
        "colab_type": "text"
      },
      "source": [
        "Running the example demonstrates that the 5×5 filter can only be applied to the 8×8 input image 4 times, resulting in a 4×4 feature map output.\n",
        "\n",
        "It may help to further develop the intuition of the relationship between filter size and the output feature map to look at two extreme cases.\n",
        "\n",
        "The first is a filter with the size of 1×1 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6ybbGftfJ94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "310f0b84-7687-4d20-91ec-99c8a18e9be9"
      },
      "source": [
        "# example of a convolutional layer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (1,1), input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 1)           2         \n",
            "=================================================================\n",
            "Total params: 2\n",
            "Trainable params: 2\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Pcn5Un4fOK3",
        "colab_type": "text"
      },
      "source": [
        "Running the example demonstrates that the output feature map has the same size as the input, specifically 8×8. This is because the filter only has a single weight (and a bias)\n",
        "\n",
        "The other extreme is a filter with the same size as the input, in this case, 8×8 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jU1iOIzifVne",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "4969a198-6973-4f7a-fa57-07b2916da56a"
      },
      "source": [
        "# example of a convolutional layer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (8,8), input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 1, 1, 1)           65        \n",
            "=================================================================\n",
            "Total params: 65\n",
            "Trainable params: 65\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pM0kUs0fYcE",
        "colab_type": "text"
      },
      "source": [
        "Running the example, we can see that, as you might expect, there is one weight for each pixel in the input image (64 + 1 for the bias) and that the output is a feature map with a single pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFq0rWmXfdUf",
        "colab_type": "text"
      },
      "source": [
        "**Fix the Border Effect Problem With Padding**\n",
        "\n",
        "By default, a filter starts at the left of the image with the left-hand side of the filter sitting on the far left pixels of the image. The filter is then stepped across the image one column at a time until the right-hand side of the filter is sitting on the far right pixels of the image.\n",
        "\n",
        "An alternative approach to applying a filter to an image is to ensure that each pixel in the image is given an opportunity to be at the center of the filter.\n",
        "\n",
        "By default, this is not the case, as the pixels on the edge of the input are only ever exposed to the edge of the filter. By starting the filter outside the frame of the image, it gives the pixels on the border of the image more of an opportunity for interacting with the filter, more of an opportunity for features to be detected by the filter, and in turn, an output feature map that has the same shape as the input image.\n",
        "\n",
        "For example, in the case of applying a 3×3 filter to the 8×8 input image, we can add a border of one pixel around the outside of the image. This has the effect of artificially creating a 10×10 input image. When the 3×3 filter is applied, it results in an 8×8 feature map. The added pixel values could have the value zero value that has no effect with the dot product operation when the filter is applied."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72gqk6anfr1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, x, x   0, 1, 0\n",
        "x, 0, 0 * 0, 1, 0 = 0\n",
        "x, 0, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0mBCEgdf85k",
        "colab_type": "text"
      },
      "source": [
        "The addition of pixels to the edge of the image is called padding.\n",
        "\n",
        "In Keras, this is specified via the “padding” argument on the Conv2D layer, which has the default value of ‘valid‘ (no padding). This means that the filter is applied only to valid ways to the input.\n",
        "\n",
        "The ‘padding‘ value of ‘same‘ calculates and adds the padding required to the input image (or feature map) to ensure that the output has the same shape as the input.\n",
        "\n",
        "The example below adds padding to the convolutional layer in our worked example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq32Bbw7f_cA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "b46aa615-aeca-413a-b9f0-0653201487af"
      },
      "source": [
        "# example a convolutional layer with padding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_7 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7wg4R4hgQFk",
        "colab_type": "text"
      },
      "source": [
        "Running the example demonstrates that the shape of the output feature map is the same as the input image: that the padding had the desired effect.\n",
        "\n",
        "The addition of padding allows the development of very deep models in such a way that the feature maps do not dwindle away to nothing.\n",
        "\n",
        "The example below demonstrates this with three stacked convolutional layers.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Wb-3RTagWQn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "0c6a04ef-7df5-4288-bc0b-64cc462e271a"
      },
      "source": [
        "# example a deep cnn with padding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), padding='same', input_shape=(8, 8, 1)))\n",
        "model.add(Conv2D(1, (3,3), padding='same'))\n",
        "model.add(Conv2D(1, (3,3), padding='same'))\n",
        "# summarize model\n",
        "model.summary()\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 8, 8, 1)           10        \n",
            "_________________________________________________________________\n",
            "conv2d_10 (Conv2D)           (None, 8, 8, 1)           10        \n",
            "=================================================================\n",
            "Total params: 30\n",
            "Trainable params: 30\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npDc5U7NgcvQ",
        "colab_type": "text"
      },
      "source": [
        "**Downsample Input With Stride**\n",
        "\n",
        "The filter is moved across the image left to right, top to bottom, with a one-pixel column change on the horizontal movements, then a one-pixel row change on the vertical movements.\n",
        "\n",
        "The amount of movement between applications of the filter to the input image is referred to as the stride, and it is almost always symmetrical in height and width dimensions.\n",
        "\n",
        "The default stride or strides in two dimensions is (1,1) for the height and the width movement, performed when needed. And this default works well in most cases.\n",
        "\n",
        "The stride can be changed, which has an effect both on how the filter is applied to the image and, in turn, the size of the resulting feature map.\n",
        "\n",
        "For example, the stride can be changed to (2,2). This has the effect of moving the filter two pixels right for each horizontal movement of the filter and two pixels down for each vertical movement of the filter when creating the feature map.\n",
        "\n",
        "We can demonstrate this with an example using the 8×8 image with a vertical line (left) dot product (“.” operator) with the vertical line filter (right) with a stride of two pixels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXW2WAaXggjN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0, 0, 0   0, 1, 0\n",
        "0, 0, 0 * 0, 1, 0 = 0\n",
        "0, 0, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HheXUhdbglH3",
        "colab_type": "text"
      },
      "source": [
        "Moved right two pixels:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkZGWAGlgpbY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "0, 1, 1   0, 1, 0\n",
        "0, 1, 1 * 0, 1, 0 = 3\n",
        "0, 1, 1   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mIqpEEfgn53",
        "colab_type": "text"
      },
      "source": [
        "Moved right two pixels:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5luw80Bgqwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "1, 0, 0   0, 1, 0\n",
        "1, 0, 0 * 0, 1, 0 = 0\n",
        "1, 0, 0   0, 1, 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-Ww8QJDg0cR",
        "colab_type": "text"
      },
      "source": [
        "We can see that there are only three valid applications of the 3×3 filters to the 8×8 input image with a stride of two. This will be the same in the vertical dimension.\n",
        "\n",
        "This has the effect of applying the filter in such a way that the normal feature map output (6×6) is down-sampled so that the size of each dimension is reduced by half (3×3), resulting in 1/4 the number of pixels (36 pixels down to 9).\n",
        "\n",
        "The stride can be specified in Keras on the Conv2D layer via the ‘stride‘ argument and specified as a tuple with height and width.\n",
        "\n",
        "The example demonstrates the application of our manual vertical line filter on the 8×8 input image with a convolutional layer that has a stride of two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjeobYJYg1d2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "294a8426-3485-4afc-8126-7b1030c2685a"
      },
      "source": [
        "# example of vertical line filter with a stride of 2\n",
        "from numpy import asarray\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D\n",
        "# define input data\n",
        "data = [[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0],\n",
        "\t\t[0, 0, 0, 1, 1, 0, 0, 0]]\n",
        "data = asarray(data)\n",
        "data = data.reshape(1, 8, 8, 1)\n",
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(1, (3,3), strides=(2, 2), input_shape=(8, 8, 1)))\n",
        "# summarize model\n",
        "model.summary()\n",
        "# define a vertical line detector\n",
        "detector = [[[[0]],[[1]],[[0]]],\n",
        "            [[[0]],[[1]],[[0]]],\n",
        "            [[[0]],[[1]],[[0]]]]\n",
        "weights = [asarray(detector), asarray([0.0])]\n",
        "# store the weights in the model\n",
        "model.set_weights(weights)\n",
        "# apply filter to input data\n",
        "yhat = model.predict(data)\n",
        "# enumerate rows\n",
        "for r in range(yhat.shape[1]):\n",
        "\t# print each column in the row\n",
        "\tprint([yhat[0,r,c,0] for c in range(yhat.shape[2])])\n",
        " "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_11 (Conv2D)           (None, 3, 3, 1)           10        \n",
            "=================================================================\n",
            "Total params: 10\n",
            "Trainable params: 10\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n",
            "[0.0, 3.0, 0.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbKjqKgHg7--",
        "colab_type": "text"
      },
      "source": [
        "Running the example, we can see from the summary of the model that the shape of the output feature map will be 3×3.\n",
        "\n",
        "Applying the handcrafted filter to the input image and printing the resulting activation feature map, we can see that, indeed, the filter still detected the vertical line, and can represent this finding with less information.\n",
        "\n",
        "Downsampling may be desirable in some cases where deeper knowledge of the filters used in the model or of the model architecture allows for some compression in the resulting feature maps."
      ]
    }
  ]
}