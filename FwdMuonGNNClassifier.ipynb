{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:pytorch]",
      "language": "python",
      "name": "conda-env-pytorch-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "FwdMuonGNNClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rysbAgzhQyOz",
        "-_VnEldJRHla",
        "jxFBp9ERwT5W"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sznajder/Lectures/blob/master/FwdMuonGNNClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vclyua3cwT4r",
        "colab_type": "text"
      },
      "source": [
        "# Graph Neural Network for hit classification\n",
        "\n",
        "### Based on:\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/README.md\n",
        "\n",
        "https://github.com/jmduarte/heptrkx-gnn-tracking/blob/master/README.md\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/MPNN_HitClassifier.ipynb\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43wElIHTwT4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "e18e9243-d62f-47d7-e09a-a63cf201d270"
      },
      "source": [
        "# System imports\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import os\n",
        "import sys\n",
        "import multiprocessing as mp\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Externals\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Torch imports\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount google drive on remote Colab machine\n",
        "drive.mount('/content/gdrive', force_remount=False)\n",
        "sys.path.append('gdrive/My Drive/Colab Notebooks')\n",
        "\n",
        "!ls 'gdrive/My Drive/Colab Notebooks/Data'\n",
        "data_dir = 'gdrive/My Drive/Colab Notebooks/Data'\n",
        "\n",
        "# Input and Output files and events to read\n",
        "infile = data_dir+'/graphs.npz'\n",
        "events_start=0\n",
        "events_end=100\n",
        "\n",
        "\n",
        "# Local imports\n",
        "#from estimator import Estimator\n",
        "#from acts import process_hits_files, select_barrel_hits\n",
        "\n",
        "%matplotlib notebook\n",
        "\n",
        "# Training concurrency\n",
        "import os\n",
        "os.environ['OMP_NUM_THREADS'] = '4'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "cuda = False\n",
        "\n",
        "if cuda:\n",
        "    np_to_torch = lambda x, volatile=False: (\n",
        "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile).cuda())\n",
        "else:\n",
        "    np_to_torch = lambda x, volatile=False: (\n",
        "        Variable(torch.from_numpy(x.astype(np.float32)), volatile=volatile))\n",
        "\n",
        "torch_to_np = lambda x: x.cpu().data.numpy()\n",
        "\n",
        "# Graph is a namedtuple of (X, Ri, Ro, y) for convenience\n",
        "feature_names = ['vh_sim_r', 'vh_sim_phi', 'vh_sim_z']\n",
        "feature_scale = np.array([1000., 180. / 6., 1000.])\n",
        "Graph = namedtuple('Graph', ['X', 'Ri', 'Ro', 'y'])\n",
        "\n",
        "# Sparse graph uses the indices for the Ri, Ro matrices\n",
        "SparseGraph = namedtuple('SparseGraph',['X', 'Ri_rows', 'Ri_cols', 'Ro_rows', 'Ro_cols', 'y'])\n",
        "\n",
        "def graph_to_sparse(graph):\n",
        "    Ri_rows, Ri_cols = graph.Ri.nonzero()\n",
        "    Ro_rows, Ro_cols = graph.Ro.nonzero()\n",
        "    return dict(X=graph.X, y=graph.y,\n",
        "                Ri_rows=Ri_rows, Ri_cols=Ri_cols,\n",
        "                Ro_rows=Ro_rows, Ro_cols=Ro_cols)\n",
        "\n",
        "def sparse_to_graph(X, Ri_rows, Ri_cols, Ro_rows, Ro_cols, y, dtype=np.uint8):\n",
        "    n_nodes, n_edges = X.shape[0], Ri_rows.shape[0]\n",
        "    Ri = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
        "    Ro = np.zeros((n_nodes, n_edges), dtype=dtype)\n",
        "    Ri[Ri_rows, Ri_cols] = 1\n",
        "    Ro[Ro_rows, Ro_cols] = 1\n",
        "    return Graph(X, Ri, Ro, y)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "_about.txt\t   graphs\t\t  ntuple_SingleMuon_Endcap_9.root\n",
            "bolsas_astro.txt   jet_images.h5\t  ntuple_SingleNeutrino_PU200_63.root\n",
            "bolsas_fisica.txt  ntuple_bkg_ZZ4mu.root  VBFHZZ_background.csv\n",
            "events.root\t   ntuple_ggH_ZZ4mu.root  VBFHZZ_signal.csv\n",
            "GOOG.csv\t   ntuple_qqH_ZZ4mu.root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rysbAgzhQyOz",
        "colab_type": "text"
      },
      "source": [
        "## PYTORCH module implementing a Message Passing GNN\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/model.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVQJbkxBQ7Lv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module implements the PyTorch modules that define the\n",
        "message-passing graph neural networks for hit or segment classification.\n",
        "\n",
        "\"\"\"\n",
        "##############################################################\n",
        "\n",
        "class EdgeNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which computes weights for edges of the graph.\n",
        "    For each edge, it selects the associated nodes' features\n",
        "    and applies some fully-connected network layers with a final\n",
        "    sigmoid activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=8, hidden_activation=nn.Tanh):\n",
        "        super(EdgeNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim*2, hidden_dim),\n",
        "            hidden_activation(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid())\n",
        "    def forward(self, X, Ri, Ro):\n",
        "        # Select the features of the associated nodes\n",
        "        bo = torch.bmm(Ro.transpose(1, 2), X)\n",
        "        bi = torch.bmm(Ri.transpose(1, 2), X)\n",
        "        B = torch.cat([bo, bi], dim=2)\n",
        "        # Apply the network to each edge\n",
        "        return self.network(B).squeeze(-1)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "class NodeNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A module which computes new node features on the graph.\n",
        "    For each node, it aggregates the neighbor node features\n",
        "    (separately on the input and output side), and combines\n",
        "    them with the node's previous features in a fully-connected\n",
        "    network to compute the new features.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim, hidden_activation=nn.Tanh):\n",
        "        super(NodeNetwork, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim*3, output_dim),\n",
        "            hidden_activation(),\n",
        "            nn.Linear(output_dim, output_dim),\n",
        "            hidden_activation())\n",
        "    def forward(self, X, e, Ri, Ro):\n",
        "        bo = torch.bmm(Ro.transpose(1, 2), X)\n",
        "        bi = torch.bmm(Ri.transpose(1, 2), X)\n",
        "        Rwo = Ro * e[:,None]\n",
        "        Rwi = Ri * e[:,None]\n",
        "        mi = torch.bmm(Rwi, bo)\n",
        "        mo = torch.bmm(Rwo, bi)\n",
        "        M = torch.cat([mi, mo, X], dim=2)\n",
        "        return self.network(M)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "class NodeClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A message-passing graph neural network model which performs\n",
        "    binary classification of nodes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=4, hidden_dim=8, n_iters=1, hidden_activation=nn.Tanh):\n",
        "        super(NodeClassifier, self).__init__()\n",
        "        self.n_iters = n_iters\n",
        "        # Setup the input network\n",
        "        self.input_network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            hidden_activation())\n",
        "        # Setup the edge network\n",
        "        self.edge_network = EdgeNetwork(input_dim+hidden_dim, hidden_dim, hidden_activation)\n",
        "        # Setup the node layers\n",
        "        self.node_network = NodeNetwork(input_dim+hidden_dim, hidden_dim, hidden_activation)\n",
        "        # Setup the output network\n",
        "        self.output_network = nn.Sequential(\n",
        "            nn.Linear(input_dim+hidden_dim, 1),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Apply forward pass of the model\"\"\"\n",
        "        X, Ri, Ro = inputs\n",
        "        # Apply input network to get hidden representation\n",
        "        H = self.input_network(X)\n",
        "        # Shortcut connect the inputs onto the hidden representation\n",
        "        H = torch.cat([H, X], dim=-1)\n",
        "        # Loop over iterations of edge and node networks\n",
        "        for i in range(self.n_iters):\n",
        "            # Apply edge network\n",
        "            e = self.edge_network(H, Ri, Ro)\n",
        "            # Apply node network\n",
        "            H = self.node_network(H, e, Ri, Ro)\n",
        "            # Shortcut connect the inputs onto the hidden representation\n",
        "            H = torch.cat([H, X], dim=-1)\n",
        "        # Apply final output network\n",
        "        return self.output_network(H).squeeze(-1)\n",
        "\n",
        "##############################################################\n",
        "\n",
        "class GNNSegmentClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Segment classification graph neural network model.\n",
        "    Consists of an input network, an edge network, and a node network.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim=2, hidden_dim=8, n_iters=3, hidden_activation=nn.Tanh):\n",
        "        super(GNNSegmentClassifier, self).__init__()\n",
        "        self.n_iters = n_iters\n",
        "        # Setup the input network\n",
        "        self.input_network = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            hidden_activation())\n",
        "        # Setup the edge network\n",
        "        self.edge_network = EdgeNetwork(input_dim+hidden_dim, hidden_dim,\n",
        "                                        hidden_activation)\n",
        "        # Setup the node layers\n",
        "        self.node_network = NodeNetwork(input_dim+hidden_dim, hidden_dim,\n",
        "                                        hidden_activation)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"Apply forward pass of the model\"\"\"\n",
        "        X, Ri, Ro = inputs\n",
        "        # Apply input network to get hidden representation\n",
        "        H = self.input_network(X)\n",
        "        # Shortcut connect the inputs onto the hidden representation\n",
        "        H = torch.cat([H, X], dim=-1)\n",
        "        # Loop over iterations of edge and node networks\n",
        "        for i in range(self.n_iters):\n",
        "            # Apply edge network\n",
        "            e = self.edge_network(H, Ri, Ro)\n",
        "            # Apply node network\n",
        "            H = self.node_network(H, e, Ri, Ro)\n",
        "            # Shortcut connect the inputs onto the hidden representation\n",
        "            H = torch.cat([H, X], dim=-1)\n",
        "        # Apply final edge network\n",
        "        return self.edge_network(H, Ri, Ro)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_VnEldJRHla",
        "colab_type": "text"
      },
      "source": [
        "## PYTORCH module implementing the Estimator\n",
        "\n",
        "https://github.com/jmduarte/gnn-fpga/blob/master/gnn/estimator.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx4_Cv00RNU-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This module contains the Estimator class implementation which provides\n",
        "code for doing the training of a PyTorch model.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import print_function\n",
        "\n",
        "from datetime import datetime\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import shutil \n",
        "import os\n",
        "\n",
        "import torch\n",
        "\n",
        "def logger(s):\n",
        "    \"\"\"Simple logger function which prints date/time\"\"\"\n",
        "    print(datetime.now(), s)\n",
        "\n",
        "class Estimator():\n",
        "    \"\"\"Estimator class\"\"\"\n",
        "\n",
        "    def __init__(self, model, loss_func, opt='Adam',\n",
        "                 train_losses=None, valid_losses=None,\n",
        "                 cuda=False, l1=0.):\n",
        "\n",
        "        self.model = model\n",
        "        if cuda:\n",
        "            self.model.cuda()\n",
        "        self.loss_func = loss_func\n",
        "        if opt == 'Adam':\n",
        "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
        "        elif opt == 'SGD':\n",
        "            self.optimizer = torch.optim.SGD(self.model.parameters())\n",
        "\n",
        "        self.train_losses = train_losses if train_losses is not None else []\n",
        "        self.valid_losses = valid_losses if valid_losses is not None else []\n",
        "        self.l1 = l1\n",
        "\n",
        "        logger('Model: \\n%s' % model)\n",
        "        logger('Parameters: %i' %\n",
        "               sum(param.numel() for param in model.parameters()))\n",
        "\n",
        "    def l1_penalty(self, arr):\n",
        "        return torch.abs(arr).sum()\n",
        "        \n",
        "    def training_step(self, inputs, targets):\n",
        "        \"\"\"Applies single optimization step on batch\"\"\"\n",
        "        self.model.zero_grad()\n",
        "        self.optimizer.zero_grad()\n",
        "        outputs = self.model(inputs)\n",
        "        node_weights = [layer.weight for layer in self.model.node_network.network if hasattr(layer, 'weight')]\n",
        "        edge_weights = [layer.weight for layer in self.model.edge_network.network if hasattr(layer, 'weight')]\n",
        "        l1_regularization = self.l1 * sum([self.l1_penalty(arr) for arr in node_weights]) + self.l1 * sum([self.l1_penalty(arr) for arr in edge_weights])\n",
        "        loss = self.loss_func(outputs, targets) + l1_regularization \n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    def save_checkpoint(self, state, is_best, filename='checkpoint.pt'):\n",
        "        directory = os.path.dirname(filename)\n",
        "        try:\n",
        "            os.stat(directory)\n",
        "        except:\n",
        "            os.mkdir(directory)\n",
        "        torch.save(state, filename)\n",
        "        if is_best:\n",
        "            bestfilename = directory+'/model_best.pt'\n",
        "            shutil.copyfile(filename, bestfilename)\n",
        "            \n",
        "    def load_checkpoint(self, filename='checkpoint.pt'):\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.model.load_state_dict(checkpoint['state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        self.valid_losses = checkpoint['valid_losses']\n",
        "        self.train_losses = checkpoint['train_losses']\n",
        "    \n",
        "    def fit_gen(self, train_generator, n_batches=1, n_epochs=1,\n",
        "                valid_generator=None, n_valid_batches=1, verbose=0, \n",
        "                filename='checkpoint.pt'):\n",
        "        \"\"\"Runs batch training for a number of specified epochs.\"\"\"\n",
        "        epoch_start = len(self.train_losses)\n",
        "        epoch_end = epoch_start + n_epochs\n",
        "        if len(self.valid_losses) > 0:\n",
        "            best_valid_loss = self.valid_losses[-1]\n",
        "        else:\n",
        "            best_valid_loss = 99999999\n",
        "        for i in range(epoch_start, epoch_end):\n",
        "            logger('Epoch %i' % i)\n",
        "            start_time = timer()\n",
        "            sum_loss = 0\n",
        "\n",
        "            # Train the model\n",
        "            self.model.train()\n",
        "            \n",
        "            for j in range(n_batches):\n",
        "                batch_input, batch_target = next(train_generator)\n",
        "                batch_loss = (self.training_step(batch_input, batch_target)\n",
        "                              .cpu().data.item())\n",
        "                sum_loss += batch_loss\n",
        "                if verbose > 0:\n",
        "                    logger('  Batch %i loss %f' % (j, batch_loss))\n",
        "            end_time = timer()\n",
        "            avg_loss = sum_loss / n_batches\n",
        "            self.train_losses.append(avg_loss)\n",
        "            logger('  training loss %.3g time %gs' %\n",
        "                   (avg_loss, (end_time - start_time)))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Evaluate the model on the validation set\n",
        "                if (valid_generator is not None) and (n_valid_batches > 0):\n",
        "                    self.model.eval()\n",
        "                    valid_loss = 0\n",
        "                    for j in range(n_valid_batches):\n",
        "                        valid_input, valid_target = next(valid_generator)\n",
        "                        valid_loss += (self.loss_func(self.model(valid_input), valid_target)\n",
        "                                       .cpu().data.item())\n",
        "                    valid_loss = valid_loss / n_valid_batches\n",
        "                    self.valid_losses.append(valid_loss)\n",
        "                    logger('  validate loss %.3g' % valid_loss)\n",
        "                \n",
        "                    #Save model checkpoint - modified\n",
        "                    logger(' save checkpoint') \n",
        "                    is_best = valid_loss < best_valid_loss\n",
        "                    best_valid_loss = min(valid_loss, best_valid_loss)\n",
        "                    self.save_checkpoint({\n",
        "                        'epoch': i + 1,\n",
        "                        'state_dict': self.model.state_dict(),\n",
        "                        'best_valid_loss': best_valid_loss,\n",
        "                        'valid_losses': self.valid_losses,\n",
        "                        'train_losses': self.train_losses,\n",
        "                        'optimizer' : self.optimizer.state_dict(),\n",
        "                    }, is_best, filename=filename)\n",
        "\n",
        "    def predict(self, generator, n_batches, concat=True):\n",
        "        with torch.no_grad():  \n",
        "            self.model.eval()\n",
        "            outputs = []\n",
        "            for j in range(n_batches):\n",
        "                test_input, test_target = next(generator)\n",
        "                outputs.append(self.model(test_input))\n",
        "            if concat:\n",
        "                outputs = torch.cat(outputs)\n",
        "            return outputs\n",
        "\n",
        "    # DEPRECATED; MOVE TO BATCH GENERATOR VERSION\n",
        "    def fit(self, train_input, train_target, batch_size=32, n_epochs=1,\n",
        "            valid_input=None, valid_target=None):\n",
        "        \"\"\"Runs batch training for a number of specified epochs.\"\"\"\n",
        "        if type(train_input) == list:\n",
        "            n_samples = train_input[0].size(0)\n",
        "        else:\n",
        "            n_samples = train_input.size(0)\n",
        "        n_batches = (n_samples + batch_size - 1) // batch_size\n",
        "        logger('Training samples: %i' % n_samples)\n",
        "        logger('Batches per epoch: %i' % n_batches)\n",
        "        if valid_input is not None:\n",
        "            n_valid = (valid_input[0].size(0) if type(valid_input) == list\n",
        "                       else valid_input.size(0))\n",
        "            logger('Validation samples: %i' % n_valid)\n",
        "\n",
        "        batch_idxs = np.arange(0, n_samples, batch_size)\n",
        "\n",
        "        epoch_start = len(self.train_losses)\n",
        "        epoch_end = epoch_start + n_epochs\n",
        "        for i in range(epoch_start, epoch_end):\n",
        "            logger('Epoch %i' % i)\n",
        "            start_time = timer()\n",
        "            sum_loss = 0\n",
        "\n",
        "            self.model.train()\n",
        "            for j in batch_idxs:\n",
        "                # TODO: add support for more customized batching\n",
        "                if type(train_input) is list:\n",
        "                    batch_input = [ti[j:j+batch_size] for ti in train_input]\n",
        "                else:\n",
        "                    batch_input = train_input[j:j+batch_size]\n",
        "                batch_target = train_target[j:j+batch_size]\n",
        "                loss = self.training_step(batch_input, batch_target)\n",
        "                sum_loss += loss.cpu().data.item()\n",
        "\n",
        "            end_time = timer()\n",
        "            avg_loss = sum_loss / n_batches\n",
        "            self.train_losses.append(avg_loss)\n",
        "            logger('  training loss %.3g time %gs' %\n",
        "                   (avg_loss, (end_time - start_time)))\n",
        "\n",
        "            # Evaluate the model on the validation set\n",
        "            self.model.eval()\n",
        "            if (valid_input is not None) and (valid_target is not None):\n",
        "                valid_loss = (self.loss_func(self.model(valid_input), valid_target)\n",
        "                              .cpu().data.item())\n",
        "                self.valid_losses.append(valid_loss)\n",
        "                logger('  validate loss %.3g' % valid_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxFBp9ERwT5W",
        "colab_type": "text"
      },
      "source": [
        "## Batch Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jU5ee5RwT5X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch_generator(X, Ri, Ro, y, n_samples=1, batch_size=1, train=True):\n",
        "    volatile = not train\n",
        "    batch_idxs = np.arange(0, n_samples, batch_size)\n",
        "    # Loop over epochs\n",
        "    while True:\n",
        "        # Loop over batches\n",
        "        for j in batch_idxs:\n",
        "            batch_X = np_to_torch(X[j:j+batch_size], volatile=volatile)\n",
        "            batch_Ri = np_to_torch(Ri[j:j+batch_size], volatile=volatile)\n",
        "            batch_Ro = np_to_torch(Ro[j:j+batch_size], volatile=volatile)\n",
        "            batch_y = np_to_torch(y[j:j+batch_size], volatile=volatile)\n",
        "            batch_inputs = [batch_X, batch_Ri, batch_Ro]\n",
        "            yield batch_inputs, batch_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIy95GB1YjJ6",
        "colab_type": "text"
      },
      "source": [
        "## Network  Model and Training Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xKxuGZBwT5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model config\n",
        "hidden_dim = 64\n",
        "n_iters = 7\n",
        "\n",
        "# Training config\n",
        "batch_size = 10\n",
        "n_epochs = 16\n",
        "valid_frac = 0.2\n",
        "test_frac = 0.2\n",
        "n_samples = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX8HZe-IbXJd",
        "colab_type": "text"
      },
      "source": [
        "## Load and prepare the graphs DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "040oVMpLbazl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "82c2d469-90e9-48a7-a294-026b23303e51"
      },
      "source": [
        "# Load GRAPHS from inputfile\n",
        "#sparse = np.load(infile, allow_pickle=True)\n",
        "#gfile = np.load(infile)\n",
        "#garray = gfile.f.arr_0 # convert file to 2D np.ndarrays\n",
        "import glob\n",
        "filenames =  glob.glob(data_dir+'/graphs/*.npz')\n",
        "\n",
        "graphs = []\n",
        "for f in filenames:\n",
        "  gfile = np.load(f)\n",
        "#  graph = SparseGraph(**dict(gfile.items()))\n",
        "  graph = Graph(**dict(gfile.items()))\n",
        "  graphs.append(graph)\n",
        "\n",
        "# Get the matrix sizes in this batch\n",
        "n_features = graphs[0].X.shape[1]\n",
        "n_nodes    = np.array([g.X.shape[0] for g in graphs])\n",
        "n_edges    = np.array([g.y.shape[0] for g in graphs])\n",
        "max_nodes = n_nodes.max()\n",
        "max_edges = n_edges.max()\n",
        "\n",
        "print(\"n_features\",n_features)\n",
        "print(\"n_nodes\",n_nodes)\n",
        "print(\"n_edges\",n_edges)\n",
        "\n",
        "# Define GRAPH tensors for the full dataset\n",
        "X  = np.zeros((n_samples, max_nodes, n_features), dtype=np.float32) # node features \n",
        "Ri = np.zeros((n_samples, max_nodes, max_edges), dtype=np.float32)  # adjacency matrix\n",
        "Ro = np.zeros((n_samples, max_nodes, max_edges), dtype=np.float32)  #\n",
        "y  = np.zeros((n_samples, max_edges), dtype=np.float32)             # target label\n",
        "\n",
        "# Loop over graphs and fill the tensors\n",
        "for i, g in enumerate(graphs):\n",
        "  X[i,  :n_nodes[i]] = g.X\n",
        "  Ri[i, :n_nodes[i], :n_edges[i]] = g.Ri\n",
        "  Ro[i, :n_nodes[i], :n_edges[i]] = g.Ro\n",
        "  y[i,  :n_edges[i]] = g.y\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_features 3\n",
            "n_nodes [ 9 19 11 24 15 13 26 10 21  9 11 12 20 18  8 18 14 10  5 12 17 23  6 13\n",
            " 10  5  9 13 13 17 14 12 11 18 12 23 15 12 11 33  8 18 15  6 13 22  4  9\n",
            " 12 12  8 10 10 22  5 12 11 15 23 18 11 11 22 11  9 13 11 11 25 24 10  9\n",
            " 21 18  5  8 18  8  8  9 17 10 11 12 29 19 11  9 22 10  9  7  8 16 10 12\n",
            " 13]\n",
            "n_edges [ 4 10  4 17  9  2  6  2 32  1  4  5  8  1  4  1  9  2  1  5  7 15  1  3\n",
            "  2  2  1  5  6  4  2  3  2  9  6  4  3  2  3 23  2 13  3  1  4 15  1  2\n",
            "  3  1  4  2  6 25  1  3  6  3  4  2  1  6 15  4  2  7  1  1 12 11  3  1\n",
            "  6  1  1  4  3  4  6  1  7  6  4  2 28  8  1  1 15  2  7  1  1  9  2  3\n",
            " 13]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coATokAiZvHF",
        "colab_type": "text"
      },
      "source": [
        "## Partition dataset into TRAIN , TEST and VALIDATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSouxwsjwT5e",
        "colab_type": "code",
        "outputId": "955f243b-0a2b-405c-82d1-b4b7947da9a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "%%time\n",
        "# We round by batch_size to avoid partial batches\n",
        "n_test = int(n_samples * test_frac) // batch_size * batch_size\n",
        "n_valid = int(n_samples * valid_frac) // batch_size * batch_size\n",
        "n_train = (n_samples - n_valid - n_test) // batch_size * batch_size\n",
        "n_train_batches = n_train // batch_size\n",
        "n_valid_batches = n_valid // batch_size\n",
        "n_test_batches = n_test // batch_size\n",
        "\n",
        "# Partition the dataset\n",
        "(train_X, test_X, train_Ri, test_Ri, train_Ro, test_Ro, train_y, test_y) = (train_test_split(X, Ri, Ro, y, test_size=n_test))\n",
        "(train_X, valid_X, train_Ri, valid_Ri, train_Ro, valid_Ro, train_y, valid_y) = (train_test_split(X, Ri, Ro, y, test_size=n_valid))\n",
        "\n",
        "# Prepare the batch samples\n",
        "train_batcher = batch_generator(train_X, train_Ri, train_Ro, train_y,\n",
        "                                n_samples=n_train, batch_size=batch_size)\n",
        "valid_batcher = batch_generator(valid_X, valid_Ri, valid_Ro, valid_y, train=False,\n",
        "                                n_samples=n_valid, batch_size=batch_size)\n",
        "test_batcher = batch_generator(test_X, test_Ri, test_Ro, test_y, train=False,\n",
        "                               n_samples=n_test, batch_size=batch_size)\n",
        "\n",
        "print(\"n_train, n_valid, n_test  = \" , n_train, \" , \" , n_valid, \" , \" , n_test )\n",
        "print('Train shapes:', train_X.shape, train_Ri.shape, train_Ro.shape, train_y.shape)\n",
        "print('Valid shapes:', valid_X.shape, valid_Ri.shape, valid_Ro.shape, valid_y.shape)\n",
        "print('Test shapes: ', test_X.shape, test_Ri.shape, test_Ro.shape, test_y.shape)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n_train, n_valid, n_test  =  60  ,  20  ,  20\n",
            "Train shapes: (80, 33, 3) (80, 33, 32) (80, 33, 32) (80, 32)\n",
            "Valid shapes: (20, 33, 3) (20, 33, 32) (20, 33, 32) (20, 32)\n",
            "Test shapes:  (20, 33, 3) (20, 33, 32) (20, 33, 32) (20, 32)\n",
            "CPU times: user 3.07 ms, sys: 1.01 ms, total: 4.08 ms\n",
            "Wall time: 7.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIY5Kr2raUdE",
        "colab_type": "text"
      },
      "source": [
        "## Construct the GNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiY4tHhQwT5k",
        "colab_type": "code",
        "outputId": "64d7d5ad-4518-435c-cd6a-dad9037f593f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        }
      },
      "source": [
        "# Construct the model\n",
        "model = NodeClassifier(input_dim=n_features, hidden_dim=hidden_dim, n_iters=n_iters)\n",
        "loss_func = nn.BCELoss()\n",
        "estim = Estimator(model, loss_func=loss_func, cuda=cuda)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-21 20:12:43.801072 Model: \n",
            "NodeClassifier(\n",
            "  (input_network): Sequential(\n",
            "    (0): Linear(in_features=3, out_features=64, bias=True)\n",
            "    (1): Tanh()\n",
            "  )\n",
            "  (edge_network): EdgeNetwork(\n",
            "    (network): Sequential(\n",
            "      (0): Linear(in_features=134, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
            "      (3): Sigmoid()\n",
            "    )\n",
            "  )\n",
            "  (node_network): NodeNetwork(\n",
            "    (network): Sequential(\n",
            "      (0): Linear(in_features=201, out_features=64, bias=True)\n",
            "      (1): Tanh()\n",
            "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
            "      (3): Tanh()\n",
            "    )\n",
            "  )\n",
            "  (output_network): Sequential(\n",
            "    (0): Linear(in_features=67, out_features=1, bias=True)\n",
            "    (1): Sigmoid()\n",
            "  )\n",
            ")\n",
            "2020-04-21 20:12:43.801312 Parameters: 26117\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27DD1v6dakaO",
        "colab_type": "text"
      },
      "source": [
        "## Train the GNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGZU0dEvwT5n",
        "colab_type": "code",
        "outputId": "7d7946d6-b0ff-4820-b007-e44e48642cf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "estim.fit_gen(train_batcher, n_batches=n_train_batches, n_epochs=n_epochs,\n",
        "              valid_generator=valid_batcher, n_valid_batches=n_valid_batches)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-04-21 20:12:43.809628 Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py:498: UserWarning: Using a target size (torch.Size([10, 32])) that is different to the input size (torch.Size([10, 33])) is deprecated. Please ensure they have the same size.\n",
            "  return F.binary_cross_entropy(input, target, weight=self.weight, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-62850436cc95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m estim.fit_gen(train_batcher, n_batches=n_train_batches, n_epochs=n_epochs,\n\u001b[0;32m----> 2\u001b[0;31m               valid_generator=valid_batcher, n_valid_batches=n_valid_batches)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-3cd1eac22a02>\u001b[0m in \u001b[0;36mfit_gen\u001b[0;34m(self, train_generator, n_batches, n_epochs, valid_generator, n_valid_batches, verbose, filename)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 batch_loss = (self.training_step(batch_input, batch_target)\n\u001b[0m\u001b[1;32m    101\u001b[0m                               .cpu().data.item())\n\u001b[1;32m    102\u001b[0m                 \u001b[0msum_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-3cd1eac22a02>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, inputs, targets)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0medge_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medge_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0ml1_regularization\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_penalty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0medge_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml1_regularization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2068\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2069\u001b[0m         raise ValueError(\"Target and input must have the same number of elements. target nelement ({}) \"\n\u001b[0;32m-> 2070\u001b[0;31m                          \"!= input nelement ({})\".format(target.numel(), input.numel()))\n\u001b[0m\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target and input must have the same number of elements. target nelement (320) != input nelement (330)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3eYkJswT5p",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USzegVFkwT5p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the loss\n",
        "plt.figure()\n",
        "plt.plot(estim.train_losses, label='training set')\n",
        "plt.plot(estim.valid_losses, label='validation set')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(loc=0);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jE8UBMojwT5r",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlxS9zT7wT5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Evaluate on TEST data\n",
        "test_pred = torch_to_np(estim.predict(test_batcher, n_test_batches))\n",
        "\n",
        "flat_y = test_y.flatten()\n",
        "flat_pred = test_pred.flatten()\n",
        "thresh = 0.5\n",
        "\n",
        "print('Test set results with threshold of', thresh)\n",
        "print('Accuracy:  %.4f' % sklearn.metrics.accuracy_score(flat_y, flat_pred>thresh))\n",
        "print('Precision: %.4f' % sklearn.metrics.precision_score(flat_y, flat_pred>thresh))\n",
        "print('Recall:    %.4f' % sklearn.metrics.recall_score(flat_y, flat_pred>thresh))\n",
        "\n",
        "# Compute the ROC curve\n",
        "fpr, tpr, _ = sklearn.metrics.roc_curve(flat_y, flat_pred)\n",
        "\n",
        "plt.figure(figsize=(9,4))\n",
        "\n",
        "# Plot the model outputs\n",
        "plt.subplot(121)\n",
        "binning=dict(bins=50, range=(0,1), histtype='bar')\n",
        "plt.hist(flat_pred[flat_y<0.5], label='fake', **binning)\n",
        "plt.hist(flat_pred[flat_y>0.5], label='true', **binning)\n",
        "plt.xlabel('Model output')\n",
        "plt.legend(loc=0)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.subplot(122)\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], [0, 1], '--')\n",
        "plt.xlabel('False positive rate')\n",
        "plt.ylabel('True positive rate')\n",
        "plt.title('ROC curve')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvpOfvzXwT5y",
        "colab_type": "text"
      },
      "source": [
        "## Visualize some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "2Ox4tJl7wT5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Draw some samples\n",
        "for i in range(4):\n",
        "    print('accuracy %.3f, precision %.3f, recall %.3f' % (\n",
        "        sklearn.metrics.accuracy_score(test_y[i], test_pred[i]>thresh),\n",
        "        sklearn.metrics.precision_score(test_y[i], test_pred[i]>thresh),\n",
        "        sklearn.metrics.recall_score(test_y[i], test_pred[i]>thresh)))\n",
        "    draw_sample(test_X[i,:,:-1]*feature_scale, test_Ri[i], test_Ro[i], test_pred[i]);"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}